{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers x-transformers betacal captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86697ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic imports\n",
    "import numpy as np  # For numerical computations and array manipulations\n",
    "import pandas as pd  # For loading and handling time-series and static data\n",
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "# PyTorch imports\n",
    "import torch  # Core PyTorch library\n",
    "import torch.nn as nn  # Neural network layers and loss functions\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "from torch.utils.data import Dataset, DataLoader  # Datasets and DataLoaders for batching\n",
    "from torch.nn import Transformer, TransformerEncoderLayer  # Transformer modules\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import IntegratedGradients\n",
    "import re\n",
    "# #Tranformers import\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, auc, roc_auc_score\n",
    "\n",
    "\n",
    "module_path = '/home/workspace/files/MilanK/Model1/final_models/code'\n",
    "# Add the module's directory to the system path if it's not already present\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "    \n",
    "\n",
    "from generating_datasets_for_torch import *\n",
    "from load_static_data import *\n",
    "from PatientDataset import *\n",
    "from generate_labels_dataframe_with_dataloader import *\n",
    "from load_train_test_split import *\n",
    "from model import *\n",
    "from load_patient_list import *\n",
    "from forward_loop import *\n",
    "from fit import *\n",
    "from validate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c576d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val,test = load_train_test_data(\n",
    "    train_filename = 'train_patient_list_orig.txt',                                   \n",
    "    val_filename = 'val_patient_list_orig.txt',\n",
    "    test_filename = 'test_patient_list.txt'\n",
    ")\n",
    "\n",
    "test_dataset = PatientDataset(patient_list = test, min_window_min=15, step_min=15,max_window_min=15,\n",
    "                             prediction_window_length=15)\n",
    "\n",
    "batch_size = 8\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    #prefetch_factor=1,\n",
    "    #persistent_workers=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062efe40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# ==============================\n",
    "# 1. YOUR DATA PREPARATION CODE\n",
    "# ==============================\n",
    "# ... e.g., define test_dataset, test_loader, model, etc. ...\n",
    "\n",
    "threshold = 0.676845\n",
    "\n",
    "# Example dynamic columns in correct order\n",
    "final_dynamic_columns = [\n",
    "    'nacooMin', 'nCoo', 'ntempEs', 'ntempSk', 'ntempCo', 'nTemp',\n",
    "    'nawRr', 'nrespRate', 'nartMean', 'nartDia', 'nartSys',\n",
    "    'nnbpMean', 'nnbpDia', 'nnbpSys', 'nPleth', 'necgRate',\n",
    "    'previous_resp_deteriorations','previous_cardiovascular_deteriorations'\n",
    "]\n",
    "\n",
    "# Example scalar columns\n",
    "scalar_column_names = [\n",
    "    'age_months',\n",
    "    'age_months_missing',\n",
    "    'gender',\n",
    "    'gender_missing',\n",
    "    'weight_kg',\n",
    "    'weight_missing',\n",
    "    'ventilation_status',\n",
    "    'ventilation_status_missing',\n",
    "    'pim3',\n",
    "    'Destination Care Area_missing',\n",
    "    'Destination Care Area_HDU (step-up / step-down unit)',\n",
    "    'Destination Care Area_ICU',\n",
    "    'Destination Care Area_NICU',\n",
    "    'Destination Care Area_PICU',\n",
    "    'Destination Care Area_Ward',\n",
    "    'Destination Care Area_Other',\n",
    "    'vasoactive_agent_used',\n",
    "    'vasoactive_agent_used_misisng',\n",
    "    'Adrenaline',\n",
    "    'Dobutamine',\n",
    "    'Dopamine',\n",
    "    'Milrinone',\n",
    "    'Noradrenaline',\n",
    "    'Prostaglandin',\n",
    "    'Vasopressin',\n",
    "    'total_vasoactive_agents',\n",
    "    'Neurological',\n",
    "    'Cardiac',\n",
    "    'Respiratory',\n",
    "    'Multi-system',\n",
    "    'Genetic / Syndrome',\n",
    "    'Metabolic / Endocrine',\n",
    "    'Haem / Onc',\n",
    "    'Other',\n",
    "    'Renal',\n",
    "    'None',\n",
    "    'preexisting_conditions_missing',\n",
    "    'total_pre_existing_condition',\n",
    "    'day_night',\n",
    "    'primary_diagnosis_missing'\n",
    "]\n",
    "\n",
    "# Feature group lists (optional)\n",
    "age_cols = [\"age_months\", \"age_months_missing\"]\n",
    "sex_cols = [\"gender\", \"gender_missing\"]\n",
    "weight_cols = [\"weight_kg\", \"weight_missing\"]\n",
    "ventilation_cols = [\"ventilation_status\", \"ventilation_status_missing\"]\n",
    "dest_area_cols = [\n",
    "    \"Destination Care Area_missing\",\n",
    "    \"Destination Care Area_HDU (step-up / step-down unit)\",\n",
    "    \"Destination Care Area_ICU\",\n",
    "    \"Destination Care Area_NICU\",\n",
    "    \"Destination Care Area_PICU\",\n",
    "    \"Destination Care Area_Ward\",\n",
    "    \"Destination Care Area_Other\"\n",
    "]\n",
    "medical_history_columns = [\n",
    "    \"Neurological\", \"Cardiac\", \"Respiratory\", \"Multi-system\", \"Genetic / Syndrome\",\n",
    "    \"Metabolic / Endocrine\", \"Haem / Onc\", \"Other\", \"Renal\", \"None\",\n",
    "    \"preexisting_conditions_missing\", \"total_pre_existing_condition\"\n",
    "]\n",
    "vasoactive_columns = [\n",
    "    \"vasoactive_agent_used\",\n",
    "    \"vasoactive_agent_used_misisng\",\n",
    "    \"Adrenaline\",\n",
    "    \"Dobutamine\",\n",
    "    \"Dopamine\",\n",
    "    \"Milrinone\",\n",
    "    \"Noradrenaline\",\n",
    "    \"Prostaglandin\",\n",
    "    \"Vasopressin\",\n",
    "    \"total_vasoactive_agents\"\n",
    "]\n",
    "\n",
    "# ===========================\n",
    "# 2. LOAD YOUR MODEL & SETUP\n",
    "# ===========================\n",
    "model = load_model_for_eval()  # your custom function\n",
    "device = torch.device('cpu')   # or \"cuda\" if GPU is available\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# Create lists for results\n",
    "results_directional = []  # For raw signed attributions\n",
    "results_magnitude = []    # For absolute attributions\n",
    "\n",
    "patient_counter = 0\n",
    "\n",
    "# ======================================\n",
    "# 3. COMPUTE INTEGRATED GRADIENTS PER BATCH\n",
    "# ======================================\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    (\n",
    "        test_dynamic,\n",
    "        test_missing,\n",
    "        _,\n",
    "        _,\n",
    "        test_scalar,\n",
    "        test_list,\n",
    "        test_label,\n",
    "        *_  # ignore any extra items from the dataset\n",
    "    ) = batch\n",
    "\n",
    "    # Move inputs to device\n",
    "    test_dynamic = test_dynamic.to(device)\n",
    "    test_missing = test_missing.to(device)\n",
    "    test_scalar = test_scalar.to(device)\n",
    "    test_list   = test_list.to(device)\n",
    "    test_label  = test_label.to(device)\n",
    "\n",
    "    # Model predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(test_dynamic, test_missing, test_scalar, test_list)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > threshold).long()\n",
    "\n",
    "    # Prepare zero baselines\n",
    "    baseline_dynamic = torch.zeros_like(test_dynamic)\n",
    "    baseline_missing = torch.zeros_like(test_missing)\n",
    "    baseline_scalar  = torch.zeros_like(test_scalar)\n",
    "    baseline_list    = torch.zeros_like(test_list)\n",
    "\n",
    "    # Calculate attributions\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs     = (test_dynamic, test_missing, test_scalar, test_list),\n",
    "        baselines  = (baseline_dynamic, baseline_missing, baseline_scalar, baseline_list),\n",
    "        target     = 0,  # or whichever class index is relevant\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    # Unpack attributions\n",
    "    dynamic_attr, mask_attr, scalar_attr, embedding_attr = attributions\n",
    "\n",
    "    # -------------------\n",
    "    # 3A. DIRECTIONAL (SIGNED)\n",
    "    # -------------------\n",
    "    # Sum across time dimension but preserve sign\n",
    "    dynamic_importance_raw = dynamic_attr.sum(dim=1)  \n",
    "    mask_importance_raw    = mask_attr.sum(dim=1)     \n",
    "    dynamic_importance_raw = dynamic_importance_raw + mask_importance_raw\n",
    "\n",
    "    # Scalars are already single-step\n",
    "    scalar_importance_raw = scalar_attr\n",
    "\n",
    "    # Embedding: sum across embedding dimension\n",
    "    embedding_importance_raw = embedding_attr.sum(dim=1)\n",
    "\n",
    "    # -------------------\n",
    "    # 3B. MAGNITUDE (ABSOLUTE)\n",
    "    # -------------------\n",
    "    dynamic_importance_mag = dynamic_attr.abs().sum(dim=1)\n",
    "    mask_importance_mag    = mask_attr.abs().sum(dim=1)\n",
    "    dynamic_importance_mag = dynamic_importance_mag + mask_importance_mag\n",
    "\n",
    "    scalar_importance_mag  = scalar_attr.abs()\n",
    "    embedding_importance_mag = embedding_attr.abs().sum(dim=1)\n",
    "\n",
    "    # Move back to CPU numpy\n",
    "    probs_np  = probs.detach().cpu().numpy()\n",
    "    preds_np  = preds.detach().cpu().numpy()\n",
    "    labels_np = test_label.detach().cpu().numpy()\n",
    "\n",
    "    dyn_raw_np       = dynamic_importance_raw.detach().cpu().numpy()\n",
    "    scalar_raw_np    = scalar_importance_raw.detach().cpu().numpy()\n",
    "    embed_raw_np     = embedding_importance_raw.detach().cpu().numpy()\n",
    "\n",
    "    dyn_mag_np       = dynamic_importance_mag.detach().cpu().numpy()\n",
    "    scalar_mag_np    = scalar_importance_mag.detach().cpu().numpy()\n",
    "    embed_mag_np     = embedding_importance_mag.detach().cpu().numpy()\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3C. BUILD PER-PATIENT DICTIONARIES\n",
    "    # -------------------------------\n",
    "    batch_size = dyn_raw_np.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        # DIRECTIONAL ROW\n",
    "        row_dict_dir = {}\n",
    "        row_dict_dir[\"patient_id\"] = patient_counter\n",
    "\n",
    "        # Dynamic features\n",
    "        for j, val in enumerate(dyn_raw_np[i]):\n",
    "            if j < len(final_dynamic_columns):\n",
    "                row_dict_dir[final_dynamic_columns[j]] = float(val)\n",
    "            else:\n",
    "                row_dict_dir[f\"dynamic_{j}\"] = float(val)\n",
    "\n",
    "        # Scalar features\n",
    "        for j, val in enumerate(scalar_raw_np[i]):\n",
    "            row_dict_dir[scalar_column_names[j]] = float(val)\n",
    "\n",
    "        # Embedding\n",
    "        row_dict_dir[\"diagnosis_embedding\"] = float(embed_raw_np[i])\n",
    "\n",
    "        # Predictions\n",
    "        row_dict_dir[\"y_prob\"]        = float(probs_np[i])\n",
    "        row_dict_dir[\"y_pred\"]        = int(preds_np[i])\n",
    "        row_dict_dir[\"y_true_cardiac\"] = int(labels_np[i][1])  # or however your label is stored\n",
    "\n",
    "        results_directional.append(row_dict_dir)\n",
    "\n",
    "        # MAGNITUDE ROW\n",
    "        row_dict_mag = {}\n",
    "        row_dict_mag[\"patient_id\"] = patient_counter\n",
    "\n",
    "        # Dynamic features\n",
    "        for j, val in enumerate(dyn_mag_np[i]):\n",
    "            if j < len(final_dynamic_columns):\n",
    "                row_dict_mag[final_dynamic_columns[j]] = float(val)\n",
    "            else:\n",
    "                row_dict_mag[f\"dynamic_{j}\"] = float(val)\n",
    "\n",
    "        # Scalar features\n",
    "        for j, val in enumerate(scalar_mag_np[i]):\n",
    "            row_dict_mag[scalar_column_names[j]] = float(val)\n",
    "\n",
    "        # Embedding\n",
    "        row_dict_mag[\"diagnosis_embedding\"] = float(embed_mag_np[i])\n",
    "\n",
    "        # Predictions\n",
    "        row_dict_mag[\"y_prob\"]        = float(probs_np[i])\n",
    "        row_dict_mag[\"y_pred\"]        = int(preds_np[i])\n",
    "        row_dict_mag[\"y_true_cardiac\"] = int(labels_np[i][1])\n",
    "\n",
    "        results_magnitude.append(row_dict_mag)\n",
    "\n",
    "        patient_counter += 1\n",
    "\n",
    "# ======================\n",
    "# 4. CREATE DATAFRAMES\n",
    "# ======================\n",
    "df_directional = pd.DataFrame(results_directional)\n",
    "df_magnitude   = pd.DataFrame(results_magnitude)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. GROUP FEATURE IMPORTANCE (OPTIONAL)\n",
    "# -------------------------------------------------------\n",
    "def aggregate_feature_groups(df):\n",
    "    \"\"\"\n",
    "    Aggregate columns into broader feature groups (e.g. sum up age columns).\n",
    "    Adjust as needed for your use-case. \n",
    "    \"\"\"\n",
    "    df[\"age\"]                  = df[age_cols].sum(axis=1)\n",
    "    df[\"sex\"]                  = df[sex_cols].sum(axis=1)\n",
    "    df[\"weight\"]               = df[weight_cols].sum(axis=1)\n",
    "    df[\"ventilation_support\"]  = df[ventilation_cols].sum(axis=1)\n",
    "    df[\"destination_care_area\"] = df[dest_area_cols].sum(axis=1)\n",
    "    df[\"medical_history\"]      = df[medical_history_columns].sum(axis=1)\n",
    "    df[\"vasoactive_agents\"]    = df[vasoactive_columns].sum(axis=1)\n",
    "\n",
    "    # Drop the old columns that were grouped\n",
    "    drop_cols = (\n",
    "        age_cols + sex_cols + weight_cols + ventilation_cols +\n",
    "        dest_area_cols + medical_history_columns + vasoactive_columns +\n",
    "        [\"primary_diagnosis_missing\"]  # if you want to drop it\n",
    "    )\n",
    "    # Only drop columns that exist\n",
    "    drop_cols = [c for c in drop_cols if c in df.columns]\n",
    "    df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "df_directional = aggregate_feature_groups(df_directional)\n",
    "df_magnitude   = aggregate_feature_groups(df_magnitude)\n",
    "\n",
    "# -----------------------------------\n",
    "# 6. NORMALIZATION UTILITIES\n",
    "# -----------------------------------\n",
    "prediction_cols = ['y_prob', 'y_pred', 'y_true_cardiac']\n",
    "\n",
    "def normalize_magnitude_rows(df, prediction_cols):\n",
    "    \"\"\"\n",
    "    Normalizes *absolute* attributions row-wise so each patient's\n",
    "    features sum to 1.\n",
    "    \"\"\"\n",
    "    feature_cols = [c for c in df.columns if c not in prediction_cols + [\"patient_id\"]]\n",
    "    row_sums = df[feature_cols].sum(axis=1)\n",
    "    df_norm_features = df[feature_cols].div(row_sums, axis=0).fillna(0)\n",
    "    df_norm = pd.concat([df[['patient_id']], df_norm_features, df[prediction_cols]], axis=1)\n",
    "    return df_norm\n",
    "\n",
    "def normalize_signed_attributions(df, prediction_cols):\n",
    "    \"\"\"\n",
    "    Normalizes *signed* attributions row-wise by the sum of absolute values,\n",
    "    so positive values stay positive, negative stay negative,\n",
    "    and sum of absolute values becomes 1 for each row.\n",
    "    \"\"\"\n",
    "    feature_cols = [c for c in df.columns if c not in prediction_cols + [\"patient_id\"]]\n",
    "    row_abs_sum = df[feature_cols].abs().sum(axis=1)\n",
    "    df_norm_features = df[feature_cols].div(row_abs_sum, axis=0).fillna(0)\n",
    "    df_norm = pd.concat([df[['patient_id']], df_norm_features, df[prediction_cols]], axis=1)\n",
    "    return df_norm\n",
    "\n",
    "# Make the normalized DataFrames\n",
    "df_magnitude_norm   = normalize_magnitude_rows(df_magnitude.copy(), prediction_cols)\n",
    "df_directional_norm = normalize_signed_attributions(df_directional.copy(), prediction_cols)\n",
    "\n",
    "# ===========================\n",
    "# 7. SAVE OR USE THE DATAFRAMES\n",
    "# ===========================\n",
    "# e.g.:\n",
    "# df_directional.to_csv(\"integrated_gradients_directional.csv\", index=False)\n",
    "# df_magnitude.to_csv(\"integrated_gradients_magnitude.csv\", index=False)\n",
    "df_magnitude_norm.to_csv(\"/home/workspace/files/MilanK/Model1/final_models/final_cardiac_models/combined_model_simpler_demographics/integrated_gradients_magnitude_norm.csv\", index=False)\n",
    "df_directional_norm.to_csv(\"/home/workspace/files/MilanK/Model1/final_models/final_cardiac_models/combined_model_simpler_demographics/integrated_gradients_directional_norm.csv\", index=False)\n",
    "\n",
    "print(\"Directional (raw) shape:\", df_directional.shape)\n",
    "print(\"Magnitude (raw) shape:\", df_magnitude.shape)\n",
    "print(\"Directional normalized shape:\", df_directional_norm.shape)\n",
    "print(\"Magnitude normalized shape:\", df_magnitude_norm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72448d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"/home/workspace/files/MilanK/Model1/final_models/final_cardiac_models/combined_model_simpler_demographics/integrated_gradients.csv\")\n",
    "df_magnitude_norm.to_csv(\"/home/workspace/files/MilanK/Model1/final_models/final_cardiac_models/combined_model_simpler_demographics/integrated_gradients_magnitude_norm.csv\", index=False)\n",
    "df_directional_norm.to_csv(\"/home/workspace/files/MilanK/Model1/final_models/final_cardiac_models/combined_model_simpler_demographics/integrated_gradients_directional_norm.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a98358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Drop metadata columns\n",
    "feature_cols = [col for col in df.columns if col not in ['patient_id', 'y_prob', 'y_pred', 'y_true_cardiac']]\n",
    "\n",
    "# Compute mean importance across all patients\n",
    "mean_importance = df[feature_cols].mean().sort_values(ascending=True)  # Ascending so most important is at the top\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 12))  # Taller figure for better label visibility\n",
    "mean_importance.plot(kind='barh')\n",
    "plt.title('Average Feature Importance Across All Patients')\n",
    "plt.xlabel('Mean Normalized Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d123e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is called `df`\n",
    "feature_cols = [col for col in df_magnitude.columns if col not in ['patient_id', 'y_prob', 'y_pred', 'y_true_cardiac']]\n",
    "\n",
    "# Split by predicted class\n",
    "df_positive = df_magnitude[df_magnitude['y_pred'] == 1]\n",
    "df_negative = df_magnitude[df_magnitude['y_pred'] == 0]\n",
    "\n",
    "# Compute mean importance for each group\n",
    "mean_importance_pos = df_positive[feature_cols].mean()\n",
    "mean_importance_neg = df_negative[feature_cols].mean()\n",
    "\n",
    "# Combine into a new DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Positive Predictions': mean_importance_pos,\n",
    "    'Negative Predictions': mean_importance_neg\n",
    "})\n",
    "\n",
    "# Sort by average importance\n",
    "comparison_df['Average'] = comparison_df.mean(axis=1)\n",
    "comparison_df = comparison_df.sort_values('Average', ascending=True).drop(columns='Average')  # Ascending so most important is on top\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "plt.figure(figsize=(10, 12))  # Wider height for vertical layout\n",
    "comparison_df.plot(kind='barh', width=0.8)\n",
    "plt.title('Feature Importance by Prediction Class')\n",
    "plt.xlabel('Mean Normalized Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Predicted Class', loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a1745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Use your df_directional, i.e. the DataFrame with signed attributions\n",
    "df_signed = df_directional.copy()\n",
    "\n",
    "# Identify feature columns (exclude patient_id, predictions, etc.)\n",
    "non_feature_cols = ['patient_id','y_prob','y_pred','y_true_cardiac']\n",
    "feature_cols = [col for col in df_signed.columns if col not in non_feature_cols]\n",
    "\n",
    "# Split by predicted class\n",
    "df_positive = df_signed[df_signed['y_pred'] == 1]\n",
    "df_negative = df_signed[df_signed['y_pred'] == 0]\n",
    "\n",
    "# Compute mean signed contribution for each feature in each group\n",
    "mean_contrib_pos = df_positive[feature_cols].mean()\n",
    "mean_contrib_neg = df_negative[feature_cols].mean()\n",
    "\n",
    "# Create a Series of the difference (pos - neg)\n",
    "diff_pos_minus_neg = mean_contrib_pos - mean_contrib_neg\n",
    "\n",
    "# Sort by difference so the most negative differences appear at the top (or bottom)\n",
    "diff_pos_minus_neg = diff_pos_minus_neg.sort_values(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.barh(diff_pos_minus_neg.index, diff_pos_minus_neg.values)\n",
    "plt.axvline(0, color='black', linewidth=1)  # Vertical line at x=0\n",
    "\n",
    "plt.title(\"Difference in Mean Signed Attribution (Positive - Negative)\")\n",
    "plt.xlabel(\"Mean Contribution Difference\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb57be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
